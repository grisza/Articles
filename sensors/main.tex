%  LaTeX support: latex@mdpi.com 
%  In case you need support, please attach all files that are necessary for compiling as well as the log file, and specify the details of your LaTeX setup (which operating system and LaTeX version / tools you are using).

% You need to save the "mdpi.cls" and "mdpi.bst" files into the same folder as this template file.

%=================================================================
\documentclass[sensors,article,submit,moreauthors,pdftex,10pt,a4paper]{mdpi} 
%
%--------------------
% Class Options:
%--------------------
% journal
%----------
% Choose between the following MDPI journals:
% actuators, admsci, aerospace, agriculture, agronomy, algorithms, animals, antibiotics, antibodies, antioxidants, applsci, arts, atmosphere, atoms, axioms, batteries, behavsci, beverages, bioengineering, biology, biomedicines, biomimetics, biomolecules, biosensors, brainsci, buildings, carbon, cancers, catalysts, cells, challenges, chemosensors, children, chromatography, climate, coatings, computation, computers, condensedmatter, cosmetics, cryptography, crystals, data, dentistry, designs, diagnostics, diseases, diversity, econometrics, economies, education, electronics, energies, entropy, environments, epigenomes, fermentation, fibers, fishes, fluids, foods, forests, futureinternet, galaxies, games, gels, genealogy, genes, geosciences, geriatrics, healthcare, horticulturae, humanities, hydrology, informatics, information, infrastructures, inorganics, insects, instruments, ijerph, ijfs, ijms, ijgi, ijtpp, inventions, jcdd, jcm, jdb, jfb, jfmk, jimaging, jof, jintelligence, jlpea, jmse, jpm, jrfm, jsan, land, languages, laws, life, literature, lubricants, machines, magnetochemistry, marinedrugs, materials, mathematics, mca, mti, medsci, medicines, membranes, metabolites, metals, microarrays, micromachines, microorganisms, minerals, molbank, molecules, mps, nanomaterials, ncrna, neonatalscreening, nutrients, particles, pathogens, pharmaceuticals, pharmaceutics, pharmacy, philosophies, photonics, plants, polymers, processes, proteomes, publications, recycling, religions, remotesensing, resources, risks, robotics, safety, scipharm, sensors, separations, sexes, sinusitis, socsci, societies, soils, sports, standards, sustainability, symmetry, systems, technologies, toxics, toxins, tropicalmed, universe, urbansci, vaccines, vetsci, viruses, water
%---------
% article
%---------
% The default type of manuscript is article, but can be replaced by: 
% addendum, article, book, bookreview, briefreport, casereport, changes, comment, commentary, communication, conceptpaper, correction, conferenceproceedings, conferencereport, expressionofconcern, meetingreport, creative, datadescriptor, discussion, editorial, essay, erratum, hypothesis, interestingimage, letter, newbookreceived, opinion, obituary, projectreport, reply, retraction, review, preprints, shortnote, supfile, technicalnote
% supfile = supplementary materials
%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g. the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.
%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.
%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. If eps figure are used, remove the option pdftex and use LaTeX and dvi2pdf.

%=================================================================
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother 
\articlenumber{x}
\doinum{10.3390/------}
\pubvolume{xx}
\pubyear{2016}
\copyrightyear{2016}
\externaleditor{Academic Editor: name}
\history{Received: date; Accepted: date; Published: date}

%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, calc, indentfirst, fancyhdr, graphicx, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, amsthm, hyphenat, natbib, hyperref, footmisc, geometry, caption, url, mdframed

\usepackage{amsfonts}
\usepackage{stmaryrd}
\usepackage{xspace}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{subcaption}

\usetikzlibrary{patterns}
\pgfplotsset{width=12cm,compat=1.14}
\usetikzlibrary{arrows,shapes,positioning,shadows,trees,shapes.geometric}
\newcommand{\degree}{\ensuremath{{}^{\circ}}\xspace}
%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Remark, Definition
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{Hybrid Orientation Based Human Limbs Motion Tracking Method}

% If this is an expanded version of a conference paper, please cite it here: enter the full citation of your conference paper, and add $\dagger$ in the end of the title of this article.
%\conference{}

% Authors, for the paper (add full first names)
\Author{Grzegorz Glonek $^{1,\dagger,*}$ and Adam Wojciechowski $^{1,\dagger}$}
% Authors, for metadata in PDF
\AuthorNames{Grzegorz Glonek and Adam Wojciechowski}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad Institute of Information Technology, Faculty of Technical Physics, Information Technology and Applied Mathematics, Lodz University of Technology, Wolczańska 215, 90-924 Lodz, Poland}

% Contact information of the corresponding author
\corres{Correspondence: grzegorz@glonek.net.pl; Tel.: +48-660-758-666}

% Current address and/or shared authorship
\firstnote{These authors contributed equally to this work.} 

% Simple summary
%\simplesumm{}

% Abstract (Do not use inserted blank lines, i.e. \\) 
\abstract{One of the key technologies that lays behind human-machine interaction is limbs motion tracking. To make the interaction effective, the motion tracking must be able to estimate precise and unambiguous position of each tracked human joint and body part. In recent years, motion tracking became very popular and broadly available for home users because of an easy access to cheap and robust tracking devices. The paper defines the novel approach to data fusion for two of such tracking devices: Microsoft Kinect v.1 and inertial measurement units (IMU). The detailed review of their working characteristics let to elaborate a new method that fuses limbs orientation data from both devices and compensates their imprecisions. The paper presents the series of performed experiments that verified the method's accuracy. This novel approach allowed improving the precision of joints positioning up to 18\%, in terms of the most relevant methods described in the literature.}

% Keywords
\keyword{data fusion; Microsoft Kinect; IMU; motion tracking}

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}
%\AMS{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data:

%\dataset{DOI number or link to the deposited data set in cases where the data set is published or set to be published separately. If the data set is submitted and will be published as a supplement to this paper in the journal Data, this field will be filled by the editors of the journal. In this case, please make sure to submit the data set as a supplement when entering your manuscript into our manuscript editorial system.}

%\datasetlicense{license under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% For Conference Proceedings Papers: add the conference title here
%\conferencetitle{}

%\setcounter{secnumdepth}{4}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Only for the journal Gels: Please place the Experimental Section after the Conclusions

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\setcounter{section}{-1} %% Remove this when starting to work on the template.
%\section{How to Use this Template}
%The template details the sections that can be used in a manuscript. Note that the order of article sections may differ from the requirements of the journal (e.g. for the positioning of the Materials and Methods section). Please check the instructions for authors page of the journal to verify the correct order. For any questions, please contact the editorial office of the journal or support@mdpi.com. For LaTeX related questions please contact Janine Daum at latex-support@mdpi.com.

\section{Introduction}

%The introduction should briefly place the study in a broad context and highlight why it is important. It should define the purpose of the work and its significance. The current state of the research field should be reviewed carefully and key publications cited. Please highlight controversial and diverging hypotheses when necessary. Finally, briefly mention the main aim of the work and highlight the principal conclusions. As far as possible, please keep the introduction comprehensible to scientists outside your particular field of research. Citing a journal paper \cite{ref-journal}. And now citing a book reference \cite{ref-book}.
Human limbs motion tracking can be defined as a process of unambiguous limbs joints spatial positioning, devoid of significant delays. Nowadays, it might be exploited within several areas, such as entertainment issues, virtual environments interaction or medical rehabilitation treatment. However, it is the system precision that determines its possible application. \\
Since 1973, when Gunnar Johanson invented his motion capture system \cite{Johansson1973}, such techniques were available mainly for professionals. However, since 2010, when Microsoft Company released Kinect v.1 controller, motion capture became available to almost everyone. Microsoft Kinect controller is an exemplary implementation of the optical, markerless motion capture system. Though it was designed for and mainly used in the field of home physical activity games, Kinect became a popular subject for many researchers, whose goal was to find other, more advanced scenarios, where this controller might be applied e.g. Lange et al. \cite{Lange2012} and Chang et al.\cite{Chang2011} designed Kinect based systems to support physical rehabilitation for people with motor disabilities. Other controllers that represents similar type of motion tracking devices are for example Primesense Carmine and ASUS Xtion.

An easy access to the motion capture has been additionally supplemented and magnified by the fact that inertial devices such as gyroscopes and accelerometers (IMU – inertial measurement units) have become an integral and mandatory part of current electronic devices. These units are built in almost every smartphone device and they are used in various applications like pedometers \cite{Huang2012, Jayalath2013}, screen rotators \cite{Pedley2013} or digital camera orientation detectors.

Inertial measurement units (IMU) are also used as measuring devices in non-optical inertial human limbs motion tracking systems i.e. XSense\footnote{Xsens Motion Technologies, \textit{Products - Xsens 3D motion tracking},\url{https://www.xsens.com/products/}}. Despite IMU, used in products available for home users, being improved continuously, their measurements precision and work limitations can be recognized as blockers for motion capture scenarios that require high accuracy of joints positioning.\\%
The broad and easy access to mentioned devices became a trigger for many researchers around the world to work on these controllers' data fusion methods that would increase the accuracy of joints positioning. Bo et al. \cite{Bo2011a} focused on the knee joint angle estimation. They proposed accelerometer and gyroscope data fusion, with linear Kalman filter, to calculate the angle, and then aligned it to the Kinect’s estimation. In this approach, Kinect's measurements are treated as reference data so that means authors treated them as aways reliable. That also means, the method can only work while Kinect measurements are available. In case of occlusion or any other disorders, proposed method will gives incorrect results. Destelle et al. \cite{Destelle2014} have proposed the method based mainly on IMU devices. In this method, the bones orientations and overall pose of the skeleton model has been estimated basing on inertial data only, and sizes of each skeleton bone were set basing on Kinect estimation. Also the skeleton placement was defined by the \emph{Spine} joint position of the Kinect's originated skeleton model. The data from the accelerometer, gyroscope and magnetometer were fused with Madgwick’s filter \cite{Madgwick2011}. In Destelle's method, the role of the Kinect was reduced to only two functions: measuring the initial length of bones and tracking the location of the user on the scene aligned to the selected joint position. The pose of the user is estimated only by IMU. Unfortunately author's didn't publish results that allows to define the accuracy of that method. Kalkbrenner et al. \cite{Kalkbrenner2014} proposed to combine together the Madgwick’s filter with the linear Kalman filter. In their approach they fused accelerometer and gyroscope data with Madgwick’s filter to estimate the selected bones orientation. Basing on this information and on the bone length, estimated by Kinect, joints positions were calculated. In Kalkbrenner's approach, bones lengths are taken from Kinect's data in every measurement frame. That means this value is dynamic and may differ few centimeters between two consecutive frames. In the last step, joints positions, estimated by Kinect simultaneously, were fused by means of linear Kalman filter with joints positions, estimated with IMU orientations. The Kalkbrenner's method is the method with the highest declared precision of the joints position estimation. Published results show the position estimation error of about $2.2cm$. In the method proposed by Feng and Murray-Smith \cite{Murray-Smith2014} the accelerometer data has been fused with Kinect measurements with the linear Kalman filter, modified by authors to work with frequency unaligned signals. Publicly available and described results of the Murray-Smith method show that the proposed approach allows to stabilize joints position estimation, at the beginning or at the end of the movement, much faster than the classical Kalman filter implementation. On the other hand, published diagrams show results in a short time period (no longer than 5s), so it is difficult to define the positioning accuracy of this method. Tannous et al. \cite{Tannous2016} proposed the extended Kalman filter to fuse bones orientations estimated both by Kinect and IMU devices. Their experiments focused on the knees flexion in fully visible position without any occlusion. The elaborated error for knee angle estimation was $6.79\degree$ for left knee and $8.98\degree$ for right knee. The last method considered by me that address similar problem of upper limb motion tracking with IMU and Kinect device is the one described in the Tian et al. \cite{Tian2015a} article. Authors of this method proposed usage of Unscented Kalman Filter (UKF) for both accelerometer and gyroscope data fusion as well as IMU and Kinect fusion. The uniqueness of proposed method is taking into consideration geometrical constraints of human pose. If, according to mentioned constraints, estimated pose is impossible to achieve, the estimation is aligned to the closest, valid value. Thanks to that approach, authors achieved deviation of rotation angle estimation lower than $20\degree$.

The analysis of the available literature shows that the majority of known data fusion methods, is based on different variants of Kalman filter (linear, extended or unscented variant), to fuse accelerometer with gyroscope data as well as IMU data with these provided by the Kinect device. Only Kalkbrenner et al. decided to fuse accelerometer and gyroscope data with the Madgwick's filter, which is designed especially for this kind of data and, regarding to the Madgwick's report \cite{Madgwick2011}, it has better accuracy and performance than the linear Kalman filter. It is also noticeable that authors of these methods are selectively taking into consideration characteristics and limitations of measurement devices. As an example, Bo et al. \cite{Bo2011a} and Destelle et al. \cite{Destelle2014} focused mainly on IMU devices and their fusion, treating Kinect device data as a reliable reference for their methods, despite the known inaccuracy of this device. Kalkbrenner et al.\cite{Kalkbrenner2014} as well as Feng and Murray-Smith\cite{Murray-Smith2014} tried to compensate some Kinect's measurement problems using proper parameters in the Kalman filter. However, they focused only on joints position estimation fluctuations, e.g. Kalkbrenner recalculated fusion parameters only when the difference in the selected joint position estimation value, between two consecutive measurement frames, was greater than $15cm$. None of the authors included in their methods any compensation of IMU and Kinect's measurement inaccuracy due to the environment conditions (the device operational temperature) or the context of the performed motion (the body rotation angle to the camera, the distance from the measurement device). The fact that this kind of factors influence noticeably the quality and accuracy of data measured by both devices, may have impact on the further data processing and on the final joint's position estimation. Authors of the discussed methods fuse the signals of both measure devices basing on joints positions and they don't take bones orientations into consideration during that process. That fact have impact on the final results as joints positions used as during the fusion includes additional, significant error caused by the non-fixed bone length. Data fusion based on bones orientation is free of this kind of error and it uses data that are measurable directly by both considered devices.

This paper presents the novel approach to Microsoft Kinect and IMU data fusion that systemically makes up for limitations of both measurement devices and compensates their imperfections. The novel orientation based, human limbs’ joints positions tracking method outperforms state-of-art hybrid systems accuracy by 11\%--18\%.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Materials and Methods}

%Materials and Methods should be described with sufficient details to allow others to replicate and build on published results. Please note that publication of your manuscript implicates that you must make all materials, data, computer code, and protocols associated with the publication available to readers. Please disclose at the submission stage any restrictions on the availability of materials or information. New methods and protocols should be described in detail while well-established methods can be briefly described and appropriately cited.

%Research manuscripts reporting large datasets that are deposited in a publicly available database should specify where the data have been deposited and provide the relevant accession numbers. If the accession numbers have not yet been obtained at the time of submission, please state that they will be provided during review. They must be provided prior to publication.

%Interventionary studies involving animals or humans, and other studies require ethical approval must list the authority that provided approval and the corresponding ethical approval code. 
\subsection{Microsoft Kinect controller characteristics}
Microsoft Kinect controller might be described as a RGB-D camera. The first version of this device, originally designed for Xbox 360 video game console, was built from two CMOS cameras and an integrated infrared (IR) projector. One of these CMOS cameras was responsible for an RGB signal and the second one was calibrated to record IR beam’s view. Considering limbs motion tracking and body gesture recognition, the most crucial part of Microsoft Kinect controller is the chip made by the PrimeSense company. All recognition algorithms that stand behind Kinect’s motion tracking, were implemented as a firmware of this processing unit. 
The Microsoft Company has not revealed any detailed description of tracking algorithms and key characteristics, so their retrieval became a subject for scientists and hobbyist \cite{Skalski2015,Gonzalez-Jorge2013,Khoshelham2012}. The good understanding of the Kinect controller constraints seems to be indispensable to create an accurate method that would fuse its data with data collected from any other source. 

In the official device specification it is claimed that the operating range of Kinect varies between $0.8m$ and $4m$ with the field of view $57\degree$ horizontally (fig. \ref{fig:kinect:range:a}) and $43\degree$ vertically (fig. \ref{fig:kinect:range:b}). At the same time, the specification does not reveal the possible heterogeneity of the measurement's accuracy. However, the experiments conducted by the authors showed that there are distance measurement nonlinearities within the Kinect’s working area. Moreover, researchers \cite{DiFilippo2015} and hobbyists\footnote{Stackoverflow Community, \textit{Precision of the kinect depth camera}, \url{https://goo.gl/x4jWoX}} reported that the operating range is different among device series.  

A comparison of fluently changing distances between Kinect and limbs' joints, collected simultaneously from Microsoft Kinect controller and Vicon\footnote{Vicon -- high accuracy, optical marker-based motion capture system.} motion capture system, showed that Kinect has tendencies to underestimate the distance in the close range and to overestimate it in the far range. Collected values (squares on fig. \ref{fig:kinect:distanceAccuracy}) allowed to elaborate distance estimation error function in a form of 3rd order polynomial, as it is presented in eq. \ref{eq:kinect:distanceErrorModel}, calculated basing on eq. \ref{eq:characteristics:kinect:distanceAccuracyPoly}, where $X$ is the matrix of sample points, $Y$ is the matrix of values in these sample points and $A$ is the coefficients matrix used in eq. \ref{eq:kinect:distanceErrorModel}. 

\begin{equation}
	f(z)=0.02z^3-0.11z^2+0.27z-0.25 
	\label{eq:kinect:distanceErrorModel}
\end{equation}

\begin{equation}
	\begin{split}
		X^TXA &= X^TY => \begin{bmatrix}
		x_1^0&x_1^1&x_1^2&x_1^3\\
		x_2^0&x_2^1&x_2^2&x_2^3\\
		x_3^0&x_3^1&x_3^2&x_3^3\\
		\dots\\
		x_n^0&x_n^1&x_n^2&x_n^3
		\end{bmatrix}^T
		\begin{bmatrix}
			x_1^0 & x_1^1 & x_1^2 & x_1^3 \\
			x_2^0 & x_2^1 & x_2^2 & x_2^3 \\
			x_3^0 & x_3^1 & x_3^2 & x_3^3 \\
			\dots\\
			x_n^0 & x_n^1 & x_n^2 & x_n^3 
		\end{bmatrix}
		\begin{bmatrix}
			a_0 \\a_1\\a_2\\a_3
		\end{bmatrix}
		=
		\begin{bmatrix}
			x_1^0 & x_1^1 & x_1^2 & x_1^3 \\
			x_2^0 & x_2^1 & x_2^2 & x_2^3 \\
			x_3^0 & x_3^1 & x_3^2 & x_3^3 \\
			\dots\\
			x_n^0 & x_n^1 & x_n^2 & x_n^3 
		\end{bmatrix}^T
		\begin{bmatrix}
			y_0 \\y_1\\y_2\\\dots\\y_n
		\end{bmatrix} 
	\end{split}
	\label{eq:characteristics:kinect:distanceAccuracyPoly}
\end{equation}

\begin{figure}[H] %Fig. 1
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
		\centering
		\input{Figure1a.tex}
		\caption{Horizontal}
		\label{fig:kinect:range:a}
	\end{subfigure} \hfill
	\begin{subfigure}[b]{0.4\textwidth}
		\centering
		\input{Figure1b.tex}
		\caption{Vertical}
		\label{fig:kinect:range:b}
	\end{subfigure} \hfill
	\begin{subfigure}[p]{0.18\textwidth}
		\hfill
		\input{Figure1.legend.tex}               
	\end{subfigure}
	\caption{Microsoft Kinect v.1 operating range diagram in horizontal (a) and vertical (b) directions}
	\label{fig:kinect:range}
\end{figure}   

Figure \ref{fig:kinect:distanceAccuracy} presents the plot of function defined with eq. \ref{eq:kinect:distanceErrorModel}. The analysis of this chart shows that Kinect’s distance estimation is optimal when the user stands about $2m$ from the device ($2m$ – $2.3m$). Outside this distance range, when Kinect works with a noticeable error, the systematic sensor depth correction should be taken into consideration.

\begin{figure}[H] %Fig. 2
	\centering	
	\input{Figure2.tex}										
	\caption{Microsoft Kinect depth measurement accuracy as a function of Kinect-joints distance.}
	\label{fig:kinect:distanceAccuracy}
\end{figure}

Such inaccuracy and its tendencies are probably caused by the algorithm implemented in the Kinect controller. Though Microsoft did not reveal any precise technical description of their method for measuring distance between the camera and objects at the scene, original patent forms \cite{patent:20100118123,patent:20100020078,patent:20080106746} owned by PrimeSense, combined with independent research results\footnote{Andreas Reichinger, \textit{Kinect pattern uncovered}, \url{https://goo.gl/DOqRxZ}}, provide some general overview of how this recognition algorithm works. It is known that the analyzed scene, in front of the Kinect controller, is enlighten with IR light dots structured pattern, which is skew symmetric, so the Kinect can work upright or upside down. Then Kinect analysis the IR pattern’s dots distortion, and basing on that it estimates the subject distance. For the depth map reconstruction, Kinect uses two techniques in parallel: dots blurriness analysis \cite{Fofi2004} and stereo-vision based on a single IR camera and a projector \cite{Rzeszotarski2006}. The achieved depth map is a foundation of a human skeleton estimation algorithm. Depth-retrieved human body parts estimations are subsequently classified into body poses by means of the machine learning approach: random decision forest algorithms as well as on object detection algorithms, such as the one proposed by Viola-Jones \cite{Shotton2008, Shotton2011a}. 

As the only source of data for that estimation process is an IR camera (RGB camera is not used at all), the observed scene must be free of any external source of the IR light. This requirement restricts the usage of Kinect in outdoor scenarios. A need for the scene isolation from any IR sources other than the Kinect itself, is the main reason why Microsoft does not recommend using two or more controllers simultaneously. Nevertheless, scientists invented and published methods on combining signals from several Kinect devices \cite{Asteriadis2013,Kitsikidis2011,Schroder2011}. It is noticeable that information gathered from the Kinect is uncomplete, due to the lack of information about joints rotation along the bone. This is the result of the skeleton model design, where each of 20 tracked joints is described as a single point.

The last major flaw of Microsoft Kinect controller is occlusion, which occurs when a part of user’s body is covered by another object or is hidden behind any other body part (self-occlusion). The occlusion by an external object seems obvious and does not require any additional explanation, however self-occlusions are less intuitive. They are connected with Kinect’s sensitivity to user’s rotation to the camera ($\alpha$ angle in fig. \ref{fig:kinect:rotationAngle}), as Microsoft recommends working with Kinect in the face off pose, which is not precisely defined. Self-performed experiments allowed observing $\alpha$ angle changes while user rotated in front of the camera. Assuming that $P_{Sh_L} = [p^K_{{Sh}_L,X} , p^K_{{Sh}_L,Z}]$ is the position of the left shoulder and $P_{Sh_R} = [p^K_{{Sh}_R,X} , p^K_{{Sh}_R,Z}]$ is the position of the right shoulder, both defined in a Kinect controller coordinating system (limited to $X$ and $Z$ axes in fig. \ref{fig:kinect:rotationAngle}), then $\alpha$ can be calculated according to eq. \ref{eq:kinect:bodyRotationAngle}.
	
\begin{figure}[H] %Fig. 3
	\centering
	\includegraphics[width=5cm]{Figure3.png}
	\caption{Rotation angle $\alpha$ between the user and Kinect}
	\label{fig:kinect:rotationAngle}
\end{figure}
	
\begin{equation}
	\label{eq:kinect:bodyRotationAngle}
	\begin{split}
		\alpha &= 
		\begin{cases} 
			atan(\frac{|p^K_{{Sh}_R,Z} - p^K_{{Sh}_L,Z}|}{|p^K_{{Sh}_R,X} - p^K_{{Sh}_L,X}|} & , |p^K_{{Sh}_R,X} - p^K_{{Sh}_L,X}| \neq 0 \\
			\frac{\Pi}{2}                                                                    & , |p^K_{{Sh}_R,X} - p^K_{{Sh}_L,X}| = 0    \\		
		\end{cases}
	\end{split}
\end{equation}
	
There are two possible strategies that Kinect uses when the occlusion happens. Either the device tries to estimate the location of the covered joint or, when it is not able to perform even rough estimation of the joint position, it stops tracking of such joint. The results of self-performed experiments show that considerable occlusions occur when the user is rotated by more than $50\degree$ ($\alpha > 50\degree$). 
	
\begin{figure}[H] %Fig. 4
	\centering
	\includegraphics[width=11cm]{Figure4.png}
	\caption{Shoulders joints tracking state in relation to body rotation angle $\alpha$}
	\label{fig:kinect:trackingVsAlpha}
\end{figure}
	
\begin{figure}[H] %Fig. 5
	\centering
	\includegraphics[width=11cm]{Figure5.png}
	\caption{Elbow angle $\beta$ estimation in relation to body rotation angle $\alpha$}
	\label{fig:kinect:betaVsAlpha}
\end{figure}
	
Charts presented in fig. \ref{fig:kinect:trackingVsAlpha} and fig. \ref{fig:kinect:betaVsAlpha} show measured joints tracking states and the elbow angle (angle $\beta$ in fig. \ref{fig:kinect:rotationAngle}) changes during the rotation respectively. In fig. \ref{fig:kinect:trackingVsAlpha} it is visible that tracking the states of both shoulders joints fluctuate between \emph{Tracked} and \emph{Interferred} states, when subject was rotated more than $50\degree$ ($\alpha > 50\degree$) in relation to Kinect’s camera. It is worth mentioning that even well visible right shoulder joint lost tracking state during that rotation. In fig. \ref{fig:kinect:betaVsAlpha} it can be noticed that estimation of the right elbow angle $\beta$ was unstable for considerable body rotation ($\alpha > 50\degree$), though the hand was fully visible all the time.
	
\subsection{IMU characteristics}
IMU devices have been in professional usage from decades. Within the scope of the current study there are two types of sensors that measure inertial forces affecting them: accelerometers and gyroscopes. All experiments presented in this paper were performed with the usage of IvenSense MPU-6050 module, which integrates both these types of inertial devices and a thermometer. Even though the accelerometer and the gyroscope were integrated on a single PCB, their data must be processed individually, as they are affected by the different set of external noises with various frequencies that must be filtered out to make them usable. IMU device used in my research can be classified according to \cite{Alexiev2013} as a industrial device. Characteristics described in the article are similar also to other devices of this class.
	
An accelerometer is an inertial sensor that measures linear forces, affecting it along device coordinating system axes. Usually, the measurements are defined in relation to the gravity force ($1g \approx 9.81  m/{s^2}$), so it is possible to estimate its linear acceleration during the motion. When the device rests, the theoretically correct measurement should be $1g$ in upward direction and $0g$ in two other directions. However, due to the external noises (mainly high frequency), the resting device measurements are oscillating around theoretical values. Analysis of these oscillations, during calibration, lets design low pass filter\footnote{The Center for Geotechnical Modeling (CGM) at UC Davis,\textit{Signal processing and filtering of raw accelerometer records}, \url{https://goo.gl/jQt49h}}\cite{Wang2011} that would be able to filter out at least major part of these high frequency noises. Accelerometers are also sensitive to operating temperature changes and its influence on sensor’s accuracy was the subject of some researches \cite{Schneider2006, Grigorie1996}. The temperature sensitivity is caused by the architecture of the sensor, which is built of several capacitors, hence the operating temperature has influence on their capacity. During the self conducted experiment the measurement of the g-force was observed when the resting device was heated and then cooled down multiple times in the temperature range $10\degree C - 50\degree C$. The results of this experiment are presented in the fig.~\ref{fig:imu:tmep}. This chart shows that the measurement tends to change from under to over estimation with the temperature change. It is also worth noticing that the measured value of the gravity is up to $4\%$ different than the expected, what may influence the further results. According to the MPU-6050 specification and results of this experiment, the neutral operating temperature for the considered module is $25\degree C$. Due to the fact that the operating temperature of IMU device, placed on a human body, rises and stabilizes at approx. $30 \degree C$, some sort of compensation is required there. Equation \ref{eq:kinect:gravityTempModel} presents the formula of the measured error.
	
\begin{equation}
	f(T) = 7*10^-7 T^3 - 5*10^-5 T^2 + 0.0022T + 0.9648
	\label{eq:kinect:gravityTempModel}
\end{equation}
	
\begin{figure}[H] %Fig. 6
	\centering
	\input{Figure6.tex}	
	\caption{Accelerometer gravity measurements in temperature range $10\degree C - 50\degree C$}
	\label{fig:imu:tmep}
\end{figure}
	
A gyroscope – the second type of inertial sensors – measures its angular velocity in $deg/s$  units. If the sensor is at the resting state, all measurements should equal 0 for each of 3 axes. However, the device suffers from the long-term bias that should be limited (ideally – removed). The analyzed noise has low frequency characteristics, thus appropriate high pass filter\footnote{Peter Cheung, \textit{Impulse response \& digital filters}, \url{https://goo.gl/GkbGHg}}, limiting its influence, should be applied. As MEMS based gyroscope is also built from multiple capacitors, their bias is influenced by the temperature as well. However, as the distortions related to the temperature have low frequency nature its influence on the data is reduced during signal filtration by high-pass filter. 
	
The last problem, identified by the authors, related to IMU devices is the incompleteness of the data provided by these units. To calculate the accurate orientation of the IMU module, data from the accelerometer and the gyroscope (orientation around corresponding axes) should be fused together. The accelerometer allows to calculate orientation around two axes, even though it measures forces along all 3 orthogonal directions. The orientation (or the rotation) around the gravity vector is unmeasurable for such device. Theoretically, the gyroscope should retrieve orientation measures around all 3 directions, however, even the filtered signal, contains some error. Its numerical integration over time results in the significant data drift that makes such measurements useless. An example of the data drift is presented in fig. \ref{fig:imu:drift}. Despite the sensor remaining motionless during the experiment, the estimations based on the numerically integrated data showed that the sensor rotated about $60\degree$ within 2 minutes (about $0.5\degree/s$).
\begin{figure}[H] %Fig. 7
	\centering
	\includegraphics[width=0.65\textwidth]{Figure7.png}
	\caption{Gyroscope drift for non-moving device}
	\label{fig:imu:drift}
\end{figure}
	
\subsection{Hybrid, orientation based, human limbs motion tracking method}
The problem that motion tracking method needs to face, is to estimate human skeleton selected joint ($j$) fused ($F$) position ($P^F_{j,t} = [p^F_{j,x}, p^F_{j,y}, p^F_{j,z}]_t$) within the reference three dimensional coordinating system, in a particular moment of time t. To solve this problem authors propose the novel, hybrid human limbs motion tracking method, based on the continuous linear fusion of skeleton bones orientations (contrary to state-of-art skeleton joints positions fusion) with the respect to the current motion context. It takes into consideration the controllers’ reliability and compensates measurement devices imperfections described in previous chapters. The data fusion method has been split into several, sequential phases presented on diagram in fig. \ref{fig:methodPhases} and described in the further part of this article. 
		
\begin{minipage}{\linewidth}
	\centering
	\begin{minipage}[b]{0.45\linewidth}
		\begin{figure}[H] %Fig. 8
			\scalebox{0.47}{
				\input{Figure8.tex}
			}
			\caption{Orientation based data fusion method schema}
			\label{fig:methodPhases}
		\end{figure}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.45\linewidth}
		\begin{figure}[H] %Fig. 9
			\includegraphics[width=0.7\textwidth]{Figure9.png}
			\caption{The hand joint simplified hierarchical model}		
			\label{fig:hybrid:jointsHierarchy}	
		\end{figure}
	\end{minipage}
\end{minipage}
		
The discussed novel data fusion method can be presented as a formula in eq. \ref{eq:fusion:formula}.
		
\begin{equation}
	P^F_{j,t} = f(A,G,T,P_{j-1,t}^K,P_{j,t}^K,\Delta t, P^F_{j-1,t}, P^K_{sh_L,t},P^K_{sh_R,t},l) 
	\label{eq:fusion:formula}
\end{equation}
		
The estimation of the selected joint $j$ position ($P^F_{j,t}$) in particular time $t$ is based on the measurements of the accelerometer ($A=[a_x,a_y,a_z]$), the gyroscope ($G=[g_x,g_y,g_z]$) and the positions of the selected joint and its parent, measured by the Kinect controller ($P^K_{j,t} = [p^K_{j,x}, p^K_{j,y}, p^K_{j,z}]_t$, $P^K_{j-1,t} = [p^K_{j,x}, p^K_{j,y}, p^K_{j,z}]_t$). Additionally, to compensate IMU measurements, the current operating temperature $T$ is taken into consideration. Also position of both shoulder joints ($ P^K_{sh_L,t},P^K_{sh_R,t}$) are used to determinate the reliability of the data and choose the right data fusion formula. The discussed method estimates the joint position with some time interval $\Delta t$ aligned to Microsoft Kinect update interval. IMU are sticked to the body surface, on the bone of lengthś $l$ between the tracked joint $j$ and its parent joint $j-1$. To define fused position $P^F_{j,t}$ in global coordination space, it has to be aligned to the fused position of its parent joint $P^F_{j-1,t}$. The simplified hierarchical model of the hand, as well as the placement of IMU are presented in fig. \ref{fig:hybrid:jointsHierarchy}.	
		
\subsubsection{Coordination space}
Both measurement devices have different local coordination spaces which are presented in the figures \ref{fig:hybrid:coordinationSpaces:kinect} and \ref{fig:hybrid:coordinationSpaces:imu}.
	
\begin{minipage}{\linewidth}
	\centering
	\begin{minipage}[b]{0.45\linewidth}
		\begin{figure}[H] %Fig. 8
			\includegraphics[width=0.5\textwidth]{kinectSpace.png}
			\caption{Microsoft Kinect coordination space.}
			\label{fig:hybrid:coordinationSpaces:kinect}
		\end{figure}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.45\linewidth}
		\begin{figure}[H] %Fig. 9
			\includegraphics[width=0.5\textwidth]{ImuSpace.png}
			\caption{Inertial measurement units coordination space.}		
			\label{fig:hybrid:coordinationSpaces:imu}	
		\end{figure}
	\end{minipage}
\end{minipage}
		
As the majority of the data processed in the method are gathered from Kinect controller, its coordination space has been choosen as the global coordination space. Due to that, inercial coordination space ($[x,y,z]^T$, fig. \ref{fig:hybrid:coordinationSpaces:imu}) need to be transformed to the global one ($[x', y', z']^T$) with the sequence of rotation by $180\degree$ around the Y axis ($R_Y(180)$) and scale ($S$) transformations according to eq. \ref{eq:hybrid:coordinationSpaces}. Definition of one common coordination space for both measurement devices is necessary to fuse their signals together.
		
\begin{equation}
	\begin{bmatrix}
		x' \\ y' \\ z'	
	\end{bmatrix}
	= R_Y(180) S \begin{bmatrix}
	x \\ y \\ z	
	\end{bmatrix} 	= 
	\begin{bmatrix}
		\cos(180)  & 0 & \sin(180) \\ 
		0          & 1 & 0         \\ 
		-\sin(180) & 0 & \cos(180) 
	\end{bmatrix} 	
	\begin{bmatrix}
		1 & 0  & 0 \\ 
		0 & -1 & 0 \\ 
		0 & 0  & 1 
	\end{bmatrix} 	
	\begin{bmatrix}
		x \\ y \\ z	
	\end{bmatrix}	= 	
	\begin{bmatrix}
		-1 & 0  & 0  \\ 
		0  & -1 & 0  \\ 
		0  & 0  & -1 
	\end{bmatrix} 		
	\begin{bmatrix}
		x \\ y \\ z	
	\end{bmatrix}
	\label{eq:hybrid:coordinationSpaces}
\end{equation}
		
\subsubsection{IMU initialization}
The goal of the IMU initialization procedure, presented in the fig. \ref{fig:hybrid:IMUCalibration}, is to calculate the data ($G, A$) correction factors matrix ($cor = [cor_A \quad cor_G]^T = [[c_ax,c_ay,c_az ]\quad[c_gx,c_gy,c_gz ]]^T $) for each IMU individually. 
	
\begin{figure}[!htb] %Fig. 10
	\scalebox{0.55}{		
		\input{Figure10.tex}
	}
	\caption{Diagram of the IMU initialization routine}
	\label{fig:hybrid:IMUCalibration}
\end{figure}
	
During this calculation, IMU modules must lay down without any motion, in the face-up position, as this is the position, in which we know exactly the expected data ($A_0=[0,0,1]$ for the accelerometer and $G_0=[0,0,0]$ for the gyroscope). However, the ideal values are impossible to achieve due to the noise that cannot be completely removed. Because of that, the calibration routine works iteratively ($s$ -- iteration index) as long as the average IMU measurement errors ($[\overline{A}\quad \overline{G}]^T = [[\overline{a_x},\overline{a_y},\overline{a_z}]\quad[\overline{g_x},\overline{g_y},\overline{g_z}]]^T$) are lower than the defined threshold ($[A_{th}\quad G_{th}]^T = [[a_{x,th},a_{y,th},a_{z,th}]\quad[g_{x,th},g_{y,th},g_{z,th}]]^T$). In this case, a relation of $[\overline{A}\quad \overline{G}]^T \le [A_{th}\quad G_{th}]^T$ is true when each element of the first matrix is lower than the corresponding element of the second matrix. As a result of this calibration method, the matrix $cor$ is calculated, which elements added to the current IMU data, allow to use data with the desired accuracy. 
		
If $[A\quad G]_{s,i}$ is an $i$--th out of the $n$ consecutive IMU device measurements for the iteration $s$, then the average error $[\overline{A}\quad \overline{G}]^T$ is calculated according to eq. \ref{eq:hybrid:IMUCalibration:1}.
		
\begin{equation}
	\begin{bmatrix} \bar{A}^T \\ \bar{G}^T \end{bmatrix}_s =
	\begin{cases}
		\frac{1}{n}\sum_{i=1}^{n}{\begin{bmatrix}A^T \\ G^T\end{bmatrix}_{s,i}} & s = 1\\
		\frac{1}{n}\sum_{i=1}^{n}{\begin{bmatrix}A^T \\ G^T\end{bmatrix}_{s,i} - \begin{bmatrix}cor_A^T\\ cor_G^T\end{bmatrix}_{s-1}} &  s > 1
	\end{cases}
	\label{eq:hybrid:IMUCalibration:1}
\end{equation}
	
Then, the data correction factors matrix $cor$, for single measurement, is defined by the eq. \ref{eq:hybrid:IMUCalibration:2}
		
\begin{equation}
	\footnotesize
	cor_s = \begin{bmatrix}cor_A^T\\ cor_G^T\end{bmatrix}_s =
	\begin{cases}
		\frac{1}{8}\left(\begin{bmatrix}A_0^T                                                                          \\ G_0^T\end{bmatrix} - \begin{bmatrix}\bar{A}^T\\ \bar{G}^T\end{bmatrix}_1\right) & s = 1\\
		cor_{s-1} - diag(1/a_{x,th},1/a_{y,th},1/a_{z,th},1/g_{x,th},1/g_{y,th},1/g_{z,th}) \left(\begin{bmatrix}A_0^T \\ G_0^T\end{bmatrix} - \begin{bmatrix}\bar{A}^T\\ \bar{G}^T\end{bmatrix}_s\right) & s > 1
	\end{cases}
	\label{eq:hybrid:IMUCalibration:2}
\end{equation}	
		
\subsubsection{IMU data fusion filters initialization}
The Madgwick filter \cite{Madgwick2011}, defined with the formula presented in eq. \ref{eq:hybrid:madgwick}, has been used as an IMU module inherent accelerometer and gyroscope data fusion method. 
		
\begin{equation}
	Q^I=m(A,G,f_m,\Delta t)
	\label{eq:hybrid:madgwick}
\end{equation}
		
This filter estimates the IMU module orientation in the quaternion form ($Q^I$), using accelerometer data ($A$), gyroscope data ($G$) and the filtration factor ($f_m$) based on the gyroscope drift characteristics. The Madgwick filter processes the IMU data with the time interval $\Delta t$. Before the filter can run on the real data, it needs to be initialized to set up its internal parameters correctly. The Madgwick filter initialization requires to run the filter multiple times on the measurements of not-moving accelerometer and gyroscope sensors. During this initialization the filtration factor $f_m$ needs to be set to the relatively high value e.g. $f_m = 2$. After the completion of the initialization process, $f_m$ should be defined according to the average Angle Random Walk (ARW) noise value $\widetilde{\omega}$ that can be calculated with the Allan’s variance\cite{FreescaleSemiconductor2015,Allan1966,Allan1987}. In case of exploited MPU-6050, IMU modules filtration factor $f_m$ is calculated according to the original article \cite{Madgwick2011} equation: $f_m = \sqrt{\frac{3}{4}\widetilde{\omega}}$, and equals $0.082$. 
		
\subsubsection{IMU and Kinect controller data noise correction}
The data noise correction is a crucial phase for the data fusion. Its goal is to remove as much noise as possible from the raw data. All methods used in this phase are related to measurement devices characteristics described earlier in this article.\\
The first step to improve the IMU module data quality is the accelerometer measurements ($A$) error compensation due to the device operating temperature ($T$). For the neutral temperature ($T_0=25\degree C$) and the correction factor $f_T= 0.0011$, the correction is defined with eq. \ref{eq:hybrid:tempCorrection}. The value of $f_T$ has been estimated in self-conducted experiments.
		
\begin{equation}
	A'=A/(1+ f_T (T-T_0))
	\label{eq:hybrid:tempCorrection}
\end{equation}
Low and high pass filtration of accelerometer and gyroscope data, is a part of the Madgwick filter implementation, so there is no necessity to repeat this process.
The second measurement device – Kinect controller – requires two data correction steps in order to improve the measurement quality: the correction of the distance estimation and smoothing the joints position estimations. The distance estimation correction is done by the function presented in eq. \ref{eq:hybrid:distanceCorrection}, which is the opposite of the distance estimation error model (eq. \ref{eq:kinect:distanceErrorModel}). The argument of this function ($z$) is directly taken from the Kinect’s distance estimation API.
\begin{equation}
	f(z)=z'=-0.02z^3+0.11z^2-0.27z+0.25
	\label{eq:hybrid:distanceCorrection}
\end{equation}
Smoothing of the Kinect’s joints positions ($P_{j,t}^K$) is necessary as coordinates fluctuations may happen, especially when occlusions occur. While there is an occlusion, position of the same joint in two consecutive frames (measurements) can differ few centimeters, which is physically and anatomically impossible, while refresh rate of the Kinect is 30Hz. Smoothing can be done by the Kinect’s firmware, but this approach gives a significant delay of the signal interpretation. The alternative approach to smoothing positions, which was exploited in the method, is a simple low pass filter. The method used the 1st order exponential filter to remove unreliable positions estimations defined with eq. \ref{eq:hybrid:positionCorrection} and filtration factor $f_{LPF} = 0.065$.
\begin{equation}
	{P'}_{j,t}^K=f_{LPF} * P_{j,t}^K+(1-f_{LPF}) * {P'}_{j,t-1}^K
	\label{eq:hybrid:positionCorrection}
\end{equation}
		
\subsubsection{IMU and Kinect synchronization}
The data fusion of any two signals requires signals alignment in the time domain. This guarantees fusion of samples that were collected in the same time -- synchronously. The goal of IMU and Kinect signals synchronization is to find the time offset $\tau$ between two data streams. To synchronize both signals, they must have the same frequency, so the IMU signal has to be downsampled from 100Hz to 30Hz, which is a nominal frequency of the Kinect controller. The downsampling has been implemented in the form of decimation, where Kinect’s new sample availability defines the time of IMU sample picking. Then, the time offset $\tau$ between the IMU signal $I$ and the Kinect signal $K$ was defined as a time offset argument of the cross-correlation algorithm that gives the maximum value of correlation between these two signals (variable $\tau_{max}$) (eq. \ref{eq:cross-cor:1} and \ref{eq:cross-cor:2}). The value of $\tau_{max}$ is added to the timestamp of IMU samples to align it to the Kinect’s signal.
		
\begin{subequations}
	\begin{align}
		(I \ast K)(\tau) & = \int_{-\infty}^{+\infty}I(t)K(t+\tau)dt\label{eq:cross-cor:1}   \\
		\tau_{max}       & = \underset{\tau}{argmax}((I \ast K)(\tau))\label{eq:cross-cor:2} 
	\end{align}
	\label{eq:cross-cor}
\end{subequations}
		
		
		
\subsubsection{IMU and Kinect data fusion}
The data fusion method processes time aligned filtered data and takes into consideration their reliability. The most serious concerns about the quality of data used within the fusion, were related to the data provided by Microsoft Kinect controller. The decision about Kinect’s data quality was based on the several information gathered during the motion. The first one is the combination of the joint tracking state and the noise level of its position estimation. A fully visible joint has its state set to \emph{Tracked} value and its position can be considered as reliable. Otherwise, if the tracking state value is set to \emph{Interferred}, the position estimation noise level must be calculated to check if the value can be treated as reliable or not. In the \emph{Interferred} state, the position of the joint is estimated basing more on the predication than direct measurements, so its accuracy might be low and values may differ significantly between consecutive frames (or in a short time period). Other parameters taken into consideration are: the value of the angle $\alpha$ between the user and the camera, as well as the information about the tracking state of both shoulder joints. Microsoft Kinect has been designed to track the motion of the human who stands frontally to the camera. If the user rotates too much, Kinect is not able to track the motion correctly anymore. During self experiments, it turned out that the maximum reliable angle between the human and the Kinect is $50\degree$. Exceeding this rotation angle results in unreliable, often random, values of all joints positions. As the value of the $\alpha$ angle is calculated using position values of both shoulder joints, both of them should be fully visible (tracking state set to \emph{Tracked}), and their position values estimations should be stable in time. This means that the angle measurement variance should be lower than $1.5\degree$, so there should be no rapid changes in shoulders position estimations, which are characteristic for the significant error in the provided data. 
		
If all of the mentioned conditions are satisfied, signals are fused according to eq.~\ref{eq:hybrid:reliableFusion}. The novelty of the presented method, in comparison with literature approaches, lies in the fact that the fused data represents skeleton bones orientations in the form of Euler angles $E = \begin{bmatrix} \phi &  \theta & \psi \end{bmatrix}$, instead of joints resulting positions. Microsoft Kinect controller provides natively information about selected bones orientation in the form of quaternion, calculated using current bone's joints positions. Used in presented method Madgwick's filter also provides IMU orientation as single quaternion. Due to the fact that orientation fusion described in this article if defined by Euler angles, quaternions from both sources need to be converted to this form before they are used. The conversion from the quaternion to the Euler angles form can be found in \cite{Dunn2011}.
		
\begin{equation} E^F_t = 
	\begin{bmatrix}  \phi^F \\  \theta^F \\  \psi^F \end{bmatrix}_t = 
	diag(w_\phi,w_\theta,w_\psi)
	\begin{bmatrix}  \phi^I \\  \theta^I \\  \psi^I \end{bmatrix}_t + 
	diag(1-w_\phi,1-w_\theta,1-w_\psi)
	\begin{bmatrix}  \phi^K \\  \theta^K \\  \psi^K \end{bmatrix}_t
	\label{eq:hybrid:reliableFusion}
\end{equation}
		
The originally elaborated weights $[w_\phi , w_\theta , w_\psi]$ describe the importance level of IMU measurements. They were based both on the device precision and the data completeness, and were defined as $[0.98, 0.05, 0.65]$ respectively. 
		
If Kinect’s data turn out to be unreliable, they cannot be fused with the IMU data. Averaged bones orientations are then estimated basing on the previously ($t-1$) fused estimation and the current change of IMU orientation, according to eq. \ref{eq:hybrid:unreliableFusion}.
				
\begin{equation} 
	\label{eq:hybrid:unreliableFusion}
	E^F_t = 
	\begin{bmatrix}  \phi^F \\  \theta^F \\  \psi^F \end{bmatrix}_t = 
	\begin{bmatrix}  \phi^F \\  \theta^F \\  \psi^F \end{bmatrix}_{t-1} +
	diag(w_\phi,w_\theta,w_\psi)\
	(\begin{bmatrix}  \phi^I \\  \theta^I \\  \psi^I \end{bmatrix}_t -
	\begin{bmatrix}  \phi^I \\  \theta^I \\  \psi^I \end{bmatrix}_{t-1})
\end{equation}
		
However, in case of Kinect data unreliability, weights $[w_\phi , w_\theta , w_\psi]$ are defined differently. In eq.~\ref{eq:hybrid:unreliableFusion} these weights were defined as $$[0.98,(1-t_{noise}/10)*0.65,(1-t_{noise}/10)*0.65]$$ where $t_{noise}$ coefficient presents the time in seconds, when Kinect stayed unreliable and its maximum accepted value is set to 10s. After this time, the estimation of the new bone orientation (only around two axes) stops, until Kinect is available again, and the method privileges the IMU signal.

Weights proposed in data fusion formulas have been defined as a result of self made research about estimating arm and forearm bones orientation by both measurement devices while the hand motion was performed only in one, selected degree of freedom. Such constraint guaranteed that possible, accidental rotations around other axes than the considered one, have no impact to gathered results.
		
\subsubsection{Joints final position estimation}
The final joint position ($P_{j,t}^F$) estimation requires the information regarding the position of the joint’s parent in the hierarchical skeleton model ($P_{j-1,t}^F$), the fixed length of the bone between the current joint and its parent ($l$) and the bone orientation estimation ($E_t^F=[\phi^F,\theta^F,\psi^F]_t$). As a result of the data fusion, estimated orientations were presented in the form of Euler angles. However, this representation of the orientation may complicate further calculations. To simplify them, the opposite conversion from Euler angles to the quaternion $Q_t^F$ is required, according to the formula described in \cite{Dunn2011}. As the used skeletal model is defined as a hierarchical structure, the newly estimated joint position must be aligned to the position of its parent joint. Taking that into consideration, the final joint position calculation procedure is described with eq. \ref{eq:hybrid:positionCalculationForm}.
		
\begin{equation} 	
	P_{j,t}^F=P_{j-1,t}^F+Q_t^F*[l,0,0]*{Q_t^F}^{-1}     
	\label{eq:hybrid:positionCalculationForm}              	
\end{equation}
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
		
%This section may be divided by subheadings. It should provide a concise and precise description of the experimental results, their interpretation as well as the experimental conclusions that can be drawn.
		
In order to verify the accuracy of the elaborated method, several experiments were performed. They were conducted with the Vicon motion capture system, as a source of ground-truth reference data. Hand movements of 3 subjects were monitored, being tracked simultaneously with Kinect controller, two IMU attached to the hand's arm and forearm bones on the skin surface, as well as Vicon tracking system. Each user had to perform 4 exercises, each of them twice with 5 repetitions per single try. Each try started with the devices synchronization in the T-Pose and then 5 repetitions of the motion began. The initial T-pose has been used as a frame of reference and joints positions are updated according to this position. Exercises focused on the horizontal and vertical hand flexion in the elbow joint, horizontal hand motion and keeping straight arm in the T-pose with no motion for at least 60s. All these exercises schemes are presented in fig. \ref{fig:results:sequences}.
		
\begin{figure}[H] %Fig. 11
	\centering
	\includegraphics[width=11cm]{Figure11.png}
	\caption{Movement sequences performed during tests}
	\label{fig:results:sequences}
\end{figure}

3 out of 4 motion trajectories used in the verification experiments can be considered as difficult according to the measurement devices characteristics.The first motion is relatively easy to track by both devices as no joints occlusion is observed there and it is fully measurable by accelerometer as well as gyroscope. The second and the third trajectiories defines motions around the axis where IMU device is not able to correctly measure current orientation and Kinect controller loses visibility of joint due to the self occlusion. The important difference between these two motions is the number of occluded joints: elbow only in the second motion and elbow and shoulder in the third. That difference have impact on Kinect's estimations reliability which is worse when shoulder is not fully visible to the controller's camera. The last motion focus on IMU's constant measurems drift around the axis perpendicular to the gravity vector. Mentioned problems allow to consider choosen motions as one of the most challanging that proposed motion tracking system can face off so it should be able to track other trajectories with similar accuracy.
		
The accuracy of 3 parameters estimations has been observed: the position of the elbow joint, the position of the wrist joint and the angle between the arm and the forearm measured in the elbow joint. Those values have been compared with the corresponding parameters estimated with Kalkbrenner’s method \cite{Kalkbrenner2014}, which, on the contrary, fused positions rather than orientations of selected joints. The position estimation error has been defined as the root mean squared error ($RMSE$) of the Euclidean distance ($d_e$) between joint’s reference position, measured by Vicon motion capture system ($P_j^V$), and joint’s position estimated by two data fusion methods. The first one is the joints position-based Kalkbrenner data fusion method ($P_j^{FP}$) and the second: the bones orientation-based fusion method ($P_j^{FO}$) proposed by the authors of this article. The $RMSE$ value has been calculated with eq. \ref{eq:results:RMSE} \cite{Armstrong1992}. The error for the elbow joint angle $\beta$ estimation has been calculated in the similar way.
		
\begin{equation}
	{RMSE}^F_j = \sqrt{\frac{1}{n}\sum_{i=1}^{n}{d_e(P^V_{j,i}, P^{Fs}_{j,i})^2}}\quad , s = \{O,P\}
	\label{eq:results:RMSE}
\end{equation}
		
The overall error of both methods, for each test exercise, has been calculated as a mean of $RMSE$ ($\overline{RMSE}$) of each motion tracking session. To compare the Kalkbrenner’s method with the authors’ novel method the ratio $r$ between the difference of $\overline{RMSE}$ of both methods to the $\overline{RMSE}$ of Kalkbrenner’s method has been calculated according to the eq. \ref{eq:results:comparison} and expressed in the form of percents. Figures \ref{fig:results:positionError:a} and \ref{fig:results:positionError:b} show the summary of  $\overline{RMSE}$ for tracked upper limb joints: the elbow and the wrist. Above each pair of bars of these charts, the value of $r$ ratio has been presented. Basing on the results presented in fig. \ref{fig:results:positionError:a} and \ref{fig:results:positionError:b}, the improvement of the proposed bones orientation-based fusion method, in reference to the best in literature -- Kalkbrenner’s method, has been noticed. However, the improvement ratio vary between joints. The error of the elbow joint position estimation has been reduced up to 18\% (from about $3.1cm$ to $2.6cm$) and the wrist joint position estimation error -- up to 16\% (from about $3.4cm$ to $2.9cm$). The difference in improvement ratio between these two joints is caused by the fact that the elbow joint is closer to the skeleton model root than the wrist joint, so it accumulates less ascending joints position estimations errors. Figure \ref{fig:results:elbowAngleError} presents the chart of $\overline{RMSE}$ of the elbow angle $\beta$ estimation (calculated similarly to eq. \ref{eq:results:RMSE}). In this value estimation the improvement has been also noticed and its ratio is up to 11\% (from about $4.6\degree$ to $4.1\degree$).
		
\begin{equation}
	r = \frac{\overline{RMSE^P_j} - \overline{RMSE^O_j}}{\overline{RMSE^P_j}}
	\label{eq:results:comparison}
\end{equation}
		
\begin{figure}[H] %Fig. 12
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\input{Figure12a.tex}
		\caption{Elbow}
		\label{fig:results:positionError:a}
	\end{subfigure} \hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\input{Figure12b.tex}
		\caption{Wrist}
		\label{fig:results:positionError:b}
	\end{subfigure}
	\caption{Average root square mean error $\overline{RMSE}$ of the elbow (a) and the wrist (b) joints position estimation}
	\label{fig:results:positionError}
\end{figure}   
		
\begin{figure}[H] %Fig. 13
	\centering
	\input{Figure13.tex}
	\caption{Average root square mean error $\overline{RMSE}$ of the elbow flexion angle $\beta$}
	\label{fig:results:elbowAngleError}
\end{figure}
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
		
The authors presented a new, orientation-based method for skeleton joints positioning, which compensates devices' imperfections and considers the context of the motion. It was tested on a~set of right hand movements, which appear demanding for measurement devices. Results were compared with those gathered from the position-based fusion method and the reference, professional, ground-truth Vicon tracking system. Presented results proof that this new, complex approach reduces joints position estimation error. The root cause of such estimation improvement, identified by the authors, is related to the bones orientation estimation instability in the position-based method, as well as the inaccuracy of bones length measurements and the lack of the fixed skeleton model definition. Moreover, authors found that the complementary fusion of the signals measured by both devices, reduces the influence of these source signals' imperfections on the final result.
In case of IMU devices, the information about their 3D space orientation (for IMU devices sticked to the body, this orientation represents also the orientation of the bone) is default and requires less complex data processing. The estimation of the joint position, based on IMU measurements, requires the additional measurement of the selected bone length and additional geometric transformations. Due to that, the imprecision of the bone orientation estimation, as well as the lack of accuracy of the bone length measurement, have an impact on the final joint position estimation error.
Considering Microsoft Kinect controller, the human body skeletal model includes information about each bone length. However, these length values are estimated continuously and may differ significantly between two consecutive measurement frames. The difference might be even of few centimeters. During experiments, the measured differences in bones length were up to $5cm$ for \emph{Tracked} state joints.  Moreover, if the occlusion doesn't occur, the estimation of the bones orientation turns out to be more stable and reliable than the joints position estimation. 
According to the mentioned characteristics, Kinect and IMU data fusion, based on the information about 3D space orientation, reduces a considerable source of the errors.

What differentiates the proposed method from the already known methods, described in the literature, is the fact that it takes into consideration characteristics of the classes of chosen measurement devices, and provides the complex compensation of detectable errors that exist in their data. It also allows to improve the quality of the source data before their fusion. It is important that many of the characteristics described in this article is closely related to the context of the performed motion. As an example, the variable accuracy of the distance measurement between Kinect and the user, or the human rotation angle to the surface of Kinect, have impact on the stability and reliability of Kinect's data.
	
The goal of the conducted research was to find the hybrid method of human limbs motion tracking, which would take into consideration elaborated characteristics and would reduce joints position estimation error significantly. The proposed method, tested on the example of the upper limbs motion, has met this goal. The results obtained by the authors' method have been compared with the results of the position-based data fusion method with the highest declared accuracy. Three parameters estimated by both methods have been compared: the elbow joint position error, the wrist joint position error and the elbow angle $\beta$ error. The set of motions (fig. \ref{fig:results:sequences}) used to test these methods, have been chosen to check how methods work when source data are poor quality due to the limitations and imperfections of measurement devices.
	
The result of the comparison of the elbow joint position estimation error is presented on the chart in fig.~\ref{fig:results:positionError:a}. It shows that the achieved improvement in this parameter estimation is up to 18\%. The comparison of the wrist joint position estimation error is presented on the chart in fig. \ref{fig:results:positionError:b} and the improvement of this parameter estimation is slightly lower than the improvement of the elbow joint position estimation error and its value is up to 16\%. The difference in these values is caused by the distance between each of these joints and the root of the skeletal model. The joint that is further from the root accumulates existing errors of all joints that are defined between the root and itself. That explains why the improvement of the estimation error is lower for the wrist than for the elbow. The chart presented in fig.~\ref{fig:results:elbowAngleError} shows the result of the comparison of the third parameter, which is the elbow angle estimation during the motion. The best improvement in this parameter estimation was close to 11\%. However, in exercises 2 and 3, which might be considered as the most difficult from the data quality detection point of view, the improvement is noticeably lower.
	
Obtained results prove that the novel data fusion approach, based on the bones orientation, might be considered as an improved alternative to the well-known, joint position-based methods.
		
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\supplementary{The following are available online at www.mdpi.com/link, Figure S1: title, Table S1: title, Video S1: title.}
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\acknowledgments{All sources of funding of the study should be disclosed. Please clearly indicate grants that you have received in support of your research work. Clearly state if you received funds for covering the costs to publish in open access.}
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{G. Glonek designed the method and experiments and analyzed the data; A.Wojciechowski contributed and supervised the work; G. Glonek and A. Wojciechowski wrote the paper.}
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\conflictofinterests{The authors declare no conflict of interest.} 
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
\abbreviations{The following abbreviations are used in this manuscript:\\
											
	\noindent 
	\begin{tabular}{@{}ll}
		RMSE & Root Mean Squared Error    \\
		IMU  & Inertial Measurement Units \\
		LPF  & Low Pass Filter            \\
	\end{tabular}}
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\appendixtitles{no} %Leave argument "no" if all appendix headings stay EMPTY (then no dot is printed after "Appendix A"). If the appendix sections contain a heading then change the argument to "yes".
%\appendixsections{multiple} %Leave argument "multiple" if there are multiple sections. Then a counter is printed ("Appendix A?). If there is only one appendix section then change the argument to ?one? and no counter is printed (?Appendix?).
%\appendix
%\section{}
%The appendix is an optional section that can contain details and data supplemental to the main text. For example, explanations of experimental details that would disrupt the flow of the main text, but nonetheless remain crucial to understanding and reproducing the research shown; figures of replicates for experiments of which representative data is shown in the main text can be added here if brief, or as Supplementary data. Mathemtaical proofs of results not central to the paper can be added as an appendix.
		
%\section{}
%All appendix sections must be cited in the main text. In the appendixes, Figures, Tables, etc. should be labeled starting with `A', e.g., Figure A1, Figure A2, etc. 
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 
\bibliographystyle{mdpi}
		
%=====================================
% References, variant A: internal bibliography
%=====================================
%\renewcommand\bibname{References}
%\begin{thebibliography}{999}
% Reference 1
%\bibitem{ref-journal}
%Lastname, F.; Author, T. The title of the cited article. {\em Journal Abbreviation} {\bf 2008}, {\em 10}, 142-149.
% Reference 2
%\bibitem{ref-book}
%Lastname, F.F.; Author, T. The title of the cited contribution. In {\em The Book Title}; Editor, F., Meditor, A., Eds.; Publishing House: City, Country, 2007; pp. 32-58.
%\end{thebibliography}
		
%=====================================
% References, variant B: external bibliography
%=====================================
\bibliography{main}
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\sampleavailability{Samples of the compounds ...... are available from the authors.}
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

